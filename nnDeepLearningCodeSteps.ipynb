{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network and Deep Code Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on Michael Nielsen's book [Neural Networks an Deep Learning](http://neuralnetworksanddeeplearning.com/index.html). It'll give step by step code processing with some comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the basic python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list **sizes** contains the number of neurons in the respective layers of the network.  For example, if the list was ```[2, 3, 1]``` then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [2, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'numpy.random.randn' usage\n",
    "\n",
    "#### note\n",
    "---\n",
    "For random samples from $N(\\mu, \\sigma^2)$, use:\n",
    "~~~~\n",
    "sigma * np.random.randn(...) + mu\n",
    "~~~~\n",
    "\n",
    "\n",
    "#### examples\n",
    "---\n",
    "~~~~\n",
    ">>> np.random.randn()\n",
    "2.1923875335537315 #random\n",
    "~~~~\n",
    "\n",
    "\n",
    "Two-by-four array of samples from N(3, 6.25):\n",
    "~~~~\n",
    ">>> 2.5 * np.random.randn(2, 4) + 3\n",
    "array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],  #random\n",
    "       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]]) #random\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = [np.random.randn(y, 1) for y in sizes[1:]]    # generate bias paramters based on number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.48724328],\n",
       "        [ 0.93406278],\n",
       "        [ 0.42375429]]), array([[ 1.27325374],\n",
       "        [ 0.06224019],\n",
       "        [-0.74347155]]), array([[ 0.17992025]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__zip()__ in conjunction with the \\* operator can be used to unzip a list:\n",
    "~~~~\n",
    ">>> x = [1, 2, 3]\n",
    ">>> y = [4, 5, 6]\n",
    ">>> zipped = zip(x, y)\n",
    ">>> zipped\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> x2, y2 = zip(*zipped)\n",
    ">>> x == list(x2) and y == list(y2)\n",
    "True\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the weights parameters are determined by *previousNumNeurons-nextNumNeurons* pair, thus use the indices with ```sizes[ : -1]``` and ```sizes[1 : ]```.\n",
    "\n",
    "It's really tricky skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.09087206, -1.02480915],\n",
       "        [ 0.73758621, -0.13678059],\n",
       "        [ 1.63535255,  0.7132227 ]]),\n",
       " array([[-0.12201119,  0.25199167,  1.48160204],\n",
       "        [ 0.08937866, -0.19738687,  1.16034127],\n",
       "        [ 0.34190555, -0.31483018,  1.76061759]]),\n",
       " array([[-0.19020864, -1.23776123, -0.77610618]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize all the initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1: ]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[ :-1], sizes[1: ])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, if we want to create a *Network* object with \n",
    "* 2 neurons in the first layer, \n",
    "* 3 neurons in the second layer, \n",
    "* 1 neuron in the final layer.\n",
    "we'd do this with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.59107265, -0.3271491 ],\n",
       "       [ 0.50645772, -1.1246387 ],\n",
       "       [ 1.44765442,  0.30688227]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[0]    # Numpy matrix storing weights connecting 1st and 2nd layers of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14067259, -1.20439729, -0.22918305]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[1]    # Numpy matrix storing weights connecting 2nd and 3rd layers of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate function sigmoid definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add *feedforward* method to the *Network* class, which, given $a$ for the network, returns the corresponding output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            '''take out the corresponding b and w for computation.'''\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning process: SGD(stochastic gradient descent) method, which is used to find out much better $w_k$, $b_l$. The idea here is not using the whole *training data set* to compute the gradient. Instead, only use one sample, i.e. the so-called **mini_batch** to compute the gradient as estimate for the whole *training data set*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```training_data```, the list of tuples ```(x, y)```.\n",
    "* ```epochs```, number of epochs to train for.\n",
    "* ```mini_batch_size```, size of the mini-batches to use when sampling. \n",
    "* ```eta```, the learning rate $\\eta$.\n",
    "* ```test_data```, used for tracking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  \n",
    "        \n",
    "        The \"training_data\" is a list of tuples \"(x, y)\" representing \n",
    "        the training inputs and the desired outputs.  \n",
    "        \n",
    "        The other non-optional parameters are self-explanatory.  \n",
    "        \n",
    "        If \"test_data\" is provided then the network will be evaluated \n",
    "        against the test data after each epoch, and partial progress printed out.  \n",
    "        This is useful for tracking progress, but slows things down substantially.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        n = len(training_data)\n",
    "        \n",
    "        # use xrange() instead of range() only involving\n",
    "        # very large range on memory-starved machine, or\n",
    "        # when all range's elements are never used.\n",
    "        for j in xrange(epochs):\n",
    "            # Shuffle the input data set, here is the training data set.\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            # Divide the training data set into groups, which here is called\n",
    "            # mini_batches. Every group 'mini_batch' with the 'mini_batch_size'\n",
    "            # is one of the element of group sets 'mini_batches'.\n",
    "            mini_batches = [\n",
    "                training_data[k : k + mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Summarize steps above, for each epoch:\n",
    "  1. randomly shuffling training data set.\n",
    "  2. partition it into mini-batches with appropriate size.\n",
    "  3. for each mini-batch:\n",
    "     apply a single step of gradient descent by ```update_mini_batch```.\n",
    "     Here, the update step will use **back propogation** to update weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        \n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
